{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLjnjEdlYykp"
      },
      "source": [
        "## Install Kaggle and download dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WP5EBkHDY4FO",
        "outputId": "f7849032-613e-4433-91e5-c81d98fc6474"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading gtzan-dataset-music-genre-classification.zip to /content\n",
            " 98% 1.19G/1.21G [00:10<00:00, 158MB/s]\n",
            "100% 1.21G/1.21G [00:10<00:00, 127MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download the GTZAN dataset\n",
        "!kaggle datasets download -d andradaolteanu/gtzan-dataset-music-genre-classification\n",
        "\n",
        "# Unzip the downloaded dataset\n",
        "!unzip -q gtzan-dataset-music-genre-classification.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yYvuh_6Z8pQ"
      },
      "source": [
        "## Load data to pandas dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "HE1wM6VFX5Zt",
        "outputId": "3c25cd41-ce0a-4d66-d192-c74da9c2b0b1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e7953438-d357-4acd-86d9-efaad0672c28\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>length</th>\n",
              "      <th>chroma_stft_mean</th>\n",
              "      <th>chroma_stft_var</th>\n",
              "      <th>rms_mean</th>\n",
              "      <th>rms_var</th>\n",
              "      <th>spectral_centroid_mean</th>\n",
              "      <th>spectral_centroid_var</th>\n",
              "      <th>spectral_bandwidth_mean</th>\n",
              "      <th>spectral_bandwidth_var</th>\n",
              "      <th>...</th>\n",
              "      <th>mfcc16_var</th>\n",
              "      <th>mfcc17_mean</th>\n",
              "      <th>mfcc17_var</th>\n",
              "      <th>mfcc18_mean</th>\n",
              "      <th>mfcc18_var</th>\n",
              "      <th>mfcc19_mean</th>\n",
              "      <th>mfcc19_var</th>\n",
              "      <th>mfcc20_mean</th>\n",
              "      <th>mfcc20_var</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>blues.00000.0.wav</td>\n",
              "      <td>66149</td>\n",
              "      <td>0.335406</td>\n",
              "      <td>0.091048</td>\n",
              "      <td>0.130405</td>\n",
              "      <td>0.003521</td>\n",
              "      <td>1773.065032</td>\n",
              "      <td>167541.630869</td>\n",
              "      <td>1972.744388</td>\n",
              "      <td>117335.771563</td>\n",
              "      <td>...</td>\n",
              "      <td>39.687145</td>\n",
              "      <td>-3.241280</td>\n",
              "      <td>36.488243</td>\n",
              "      <td>0.722209</td>\n",
              "      <td>38.099152</td>\n",
              "      <td>-5.050335</td>\n",
              "      <td>33.618073</td>\n",
              "      <td>-0.243027</td>\n",
              "      <td>43.771767</td>\n",
              "      <td>blues</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>blues.00000.1.wav</td>\n",
              "      <td>66149</td>\n",
              "      <td>0.343065</td>\n",
              "      <td>0.086147</td>\n",
              "      <td>0.112699</td>\n",
              "      <td>0.001450</td>\n",
              "      <td>1816.693777</td>\n",
              "      <td>90525.690866</td>\n",
              "      <td>2010.051501</td>\n",
              "      <td>65671.875673</td>\n",
              "      <td>...</td>\n",
              "      <td>64.748276</td>\n",
              "      <td>-6.055294</td>\n",
              "      <td>40.677654</td>\n",
              "      <td>0.159015</td>\n",
              "      <td>51.264091</td>\n",
              "      <td>-2.837699</td>\n",
              "      <td>97.030830</td>\n",
              "      <td>5.784063</td>\n",
              "      <td>59.943081</td>\n",
              "      <td>blues</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>blues.00000.2.wav</td>\n",
              "      <td>66149</td>\n",
              "      <td>0.346815</td>\n",
              "      <td>0.092243</td>\n",
              "      <td>0.132003</td>\n",
              "      <td>0.004620</td>\n",
              "      <td>1788.539719</td>\n",
              "      <td>111407.437613</td>\n",
              "      <td>2084.565132</td>\n",
              "      <td>75124.921716</td>\n",
              "      <td>...</td>\n",
              "      <td>67.336563</td>\n",
              "      <td>-1.768610</td>\n",
              "      <td>28.348579</td>\n",
              "      <td>2.378768</td>\n",
              "      <td>45.717648</td>\n",
              "      <td>-1.938424</td>\n",
              "      <td>53.050835</td>\n",
              "      <td>2.517375</td>\n",
              "      <td>33.105122</td>\n",
              "      <td>blues</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>blues.00000.3.wav</td>\n",
              "      <td>66149</td>\n",
              "      <td>0.363639</td>\n",
              "      <td>0.086856</td>\n",
              "      <td>0.132565</td>\n",
              "      <td>0.002448</td>\n",
              "      <td>1655.289045</td>\n",
              "      <td>111952.284517</td>\n",
              "      <td>1960.039988</td>\n",
              "      <td>82913.639269</td>\n",
              "      <td>...</td>\n",
              "      <td>47.739452</td>\n",
              "      <td>-3.841155</td>\n",
              "      <td>28.337118</td>\n",
              "      <td>1.218588</td>\n",
              "      <td>34.770935</td>\n",
              "      <td>-3.580352</td>\n",
              "      <td>50.836224</td>\n",
              "      <td>3.630866</td>\n",
              "      <td>32.023678</td>\n",
              "      <td>blues</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>blues.00000.4.wav</td>\n",
              "      <td>66149</td>\n",
              "      <td>0.335579</td>\n",
              "      <td>0.088129</td>\n",
              "      <td>0.143289</td>\n",
              "      <td>0.001701</td>\n",
              "      <td>1630.656199</td>\n",
              "      <td>79667.267654</td>\n",
              "      <td>1948.503884</td>\n",
              "      <td>60204.020268</td>\n",
              "      <td>...</td>\n",
              "      <td>30.336359</td>\n",
              "      <td>0.664582</td>\n",
              "      <td>45.880913</td>\n",
              "      <td>1.689446</td>\n",
              "      <td>51.363583</td>\n",
              "      <td>-3.392489</td>\n",
              "      <td>26.738789</td>\n",
              "      <td>0.536961</td>\n",
              "      <td>29.146694</td>\n",
              "      <td>blues</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 60 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e7953438-d357-4acd-86d9-efaad0672c28')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e7953438-d357-4acd-86d9-efaad0672c28 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e7953438-d357-4acd-86d9-efaad0672c28');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-82bdfc6c-37ae-45d3-9584-b9f79debb93c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-82bdfc6c-37ae-45d3-9584-b9f79debb93c')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-82bdfc6c-37ae-45d3-9584-b9f79debb93c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "            filename  length  chroma_stft_mean  chroma_stft_var  rms_mean  \\\n",
              "0  blues.00000.0.wav   66149          0.335406         0.091048  0.130405   \n",
              "1  blues.00000.1.wav   66149          0.343065         0.086147  0.112699   \n",
              "2  blues.00000.2.wav   66149          0.346815         0.092243  0.132003   \n",
              "3  blues.00000.3.wav   66149          0.363639         0.086856  0.132565   \n",
              "4  blues.00000.4.wav   66149          0.335579         0.088129  0.143289   \n",
              "\n",
              "    rms_var  spectral_centroid_mean  spectral_centroid_var  \\\n",
              "0  0.003521             1773.065032          167541.630869   \n",
              "1  0.001450             1816.693777           90525.690866   \n",
              "2  0.004620             1788.539719          111407.437613   \n",
              "3  0.002448             1655.289045          111952.284517   \n",
              "4  0.001701             1630.656199           79667.267654   \n",
              "\n",
              "   spectral_bandwidth_mean  spectral_bandwidth_var  ...  mfcc16_var  \\\n",
              "0              1972.744388           117335.771563  ...   39.687145   \n",
              "1              2010.051501            65671.875673  ...   64.748276   \n",
              "2              2084.565132            75124.921716  ...   67.336563   \n",
              "3              1960.039988            82913.639269  ...   47.739452   \n",
              "4              1948.503884            60204.020268  ...   30.336359   \n",
              "\n",
              "   mfcc17_mean  mfcc17_var  mfcc18_mean  mfcc18_var  mfcc19_mean  mfcc19_var  \\\n",
              "0    -3.241280   36.488243     0.722209   38.099152    -5.050335   33.618073   \n",
              "1    -6.055294   40.677654     0.159015   51.264091    -2.837699   97.030830   \n",
              "2    -1.768610   28.348579     2.378768   45.717648    -1.938424   53.050835   \n",
              "3    -3.841155   28.337118     1.218588   34.770935    -3.580352   50.836224   \n",
              "4     0.664582   45.880913     1.689446   51.363583    -3.392489   26.738789   \n",
              "\n",
              "   mfcc20_mean  mfcc20_var  label  \n",
              "0    -0.243027   43.771767  blues  \n",
              "1     5.784063   59.943081  blues  \n",
              "2     2.517375   33.105122  blues  \n",
              "3     3.630866   32.023678  blues  \n",
              "4     0.536961   29.146694  blues  \n",
              "\n",
              "[5 rows x 60 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load data to dataframe\n",
        "data_path = '/content/Data/features_3_sec.csv'\n",
        "data = pd.read_csv(data_path)\n",
        "\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvH93rpTaCAa"
      },
      "source": [
        "## Split the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGlvVZikaSTc",
        "outputId": "51837ae4-a98d-4928-9d4c-4c2a9548f6b6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((7992, 58), (1998, 58), (7992,), (1998,))"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "seed = 42\n",
        "\n",
        "# Splitting data into features and labels\n",
        "X = data.drop(['filename', 'label'], axis=1)\n",
        "y = data['label']\n",
        "\n",
        "# Encoding labels and feature normalization\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(y)\n",
        "X = StandardScaler().fit_transform(X)\n",
        "\n",
        "# Split data into training and validation parts\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFlEnMKmbyQu"
      },
      "source": [
        "## Deep Learning Approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKymVUHIMddh"
      },
      "source": [
        "### DP model without dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hasAl2KVb4g_",
        "outputId": "62792100-86c8-4d2b-b17f-486b818fa61b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 512)               30208     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 203338 (794.29 KB)\n",
            "Trainable params: 203338 (794.29 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/150\n",
            "250/250 [==============================] - 12s 14ms/step - loss: 1.0559 - accuracy: 0.6306 - val_loss: 0.7458 - val_accuracy: 0.7452\n",
            "Epoch 2/150\n",
            "250/250 [==============================] - 2s 8ms/step - loss: 0.5877 - accuracy: 0.7969 - val_loss: 0.6037 - val_accuracy: 0.7948\n",
            "Epoch 3/150\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 0.4199 - accuracy: 0.8591 - val_loss: 0.4876 - val_accuracy: 0.8308\n",
            "Epoch 4/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.2958 - accuracy: 0.8963 - val_loss: 0.4216 - val_accuracy: 0.8639\n",
            "Epoch 5/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.2143 - accuracy: 0.9272 - val_loss: 0.4795 - val_accuracy: 0.8564\n",
            "Epoch 6/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.1612 - accuracy: 0.9463 - val_loss: 0.4678 - val_accuracy: 0.8639\n",
            "Epoch 7/150\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 0.1259 - accuracy: 0.9547 - val_loss: 0.3941 - val_accuracy: 0.8874\n",
            "Epoch 8/150\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 0.1223 - accuracy: 0.9578 - val_loss: 0.3912 - val_accuracy: 0.8854\n",
            "Epoch 9/150\n",
            "250/250 [==============================] - 3s 13ms/step - loss: 0.0888 - accuracy: 0.9718 - val_loss: 0.4005 - val_accuracy: 0.8934\n",
            "Epoch 10/150\n",
            "250/250 [==============================] - 2s 8ms/step - loss: 0.0606 - accuracy: 0.9802 - val_loss: 0.3858 - val_accuracy: 0.8989\n",
            "Epoch 11/150\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 0.0769 - accuracy: 0.9721 - val_loss: 0.5037 - val_accuracy: 0.8674\n",
            "Epoch 12/150\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 0.0695 - accuracy: 0.9779 - val_loss: 0.4954 - val_accuracy: 0.8744\n",
            "Epoch 13/150\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 0.0933 - accuracy: 0.9702 - val_loss: 0.3876 - val_accuracy: 0.9004\n",
            "Epoch 14/150\n",
            "250/250 [==============================] - 3s 11ms/step - loss: 0.0428 - accuracy: 0.9845 - val_loss: 0.3901 - val_accuracy: 0.9009\n",
            "Epoch 15/150\n",
            "250/250 [==============================] - 2s 8ms/step - loss: 0.0699 - accuracy: 0.9817 - val_loss: 0.4134 - val_accuracy: 0.8949\n",
            "Epoch 16/150\n",
            "250/250 [==============================] - 2s 8ms/step - loss: 0.0525 - accuracy: 0.9829 - val_loss: 0.4790 - val_accuracy: 0.8884\n",
            "Epoch 17/150\n",
            "250/250 [==============================] - 2s 8ms/step - loss: 0.0461 - accuracy: 0.9846 - val_loss: 0.5234 - val_accuracy: 0.8769\n",
            "Epoch 18/150\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.0483 - accuracy: 0.9839 - val_loss: 0.4519 - val_accuracy: 0.8959\n",
            "Epoch 19/150\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.0352 - accuracy: 0.9880 - val_loss: 0.4025 - val_accuracy: 0.9054\n",
            "Epoch 20/150\n",
            "250/250 [==============================] - 3s 11ms/step - loss: 0.0677 - accuracy: 0.9789 - val_loss: 0.5081 - val_accuracy: 0.8799\n",
            "Epoch 21/150\n",
            "250/250 [==============================] - 2s 8ms/step - loss: 0.0519 - accuracy: 0.9831 - val_loss: 0.3457 - val_accuracy: 0.9234\n",
            "Epoch 22/150\n",
            "250/250 [==============================] - 2s 8ms/step - loss: 0.0085 - accuracy: 0.9970 - val_loss: 0.4009 - val_accuracy: 0.9219\n",
            "Epoch 23/150\n",
            "250/250 [==============================] - 2s 6ms/step - loss: 0.0249 - accuracy: 0.9937 - val_loss: 0.4845 - val_accuracy: 0.9129\n",
            "Epoch 24/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0542 - accuracy: 0.9834 - val_loss: 0.6313 - val_accuracy: 0.8554\n",
            "Epoch 25/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0718 - accuracy: 0.9775 - val_loss: 0.4223 - val_accuracy: 0.8954\n",
            "Epoch 26/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0376 - accuracy: 0.9875 - val_loss: 0.4244 - val_accuracy: 0.9059\n",
            "Epoch 27/150\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 0.0602 - accuracy: 0.9822 - val_loss: 0.3710 - val_accuracy: 0.9104\n",
            "Epoch 28/150\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 0.0400 - accuracy: 0.9885 - val_loss: 0.4459 - val_accuracy: 0.9069\n",
            "Epoch 29/150\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 0.0197 - accuracy: 0.9942 - val_loss: 0.4321 - val_accuracy: 0.9144\n",
            "Epoch 30/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0238 - accuracy: 0.9930 - val_loss: 0.4196 - val_accuracy: 0.9089\n",
            "Epoch 31/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0306 - accuracy: 0.9914 - val_loss: 0.4214 - val_accuracy: 0.9089\n",
            "Epoch 32/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0393 - accuracy: 0.9892 - val_loss: 0.4782 - val_accuracy: 0.8944\n",
            "Epoch 33/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0501 - accuracy: 0.9855 - val_loss: 0.4322 - val_accuracy: 0.9034\n",
            "Epoch 34/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0442 - accuracy: 0.9861 - val_loss: 0.4666 - val_accuracy: 0.9004\n",
            "Epoch 35/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0304 - accuracy: 0.9915 - val_loss: 0.3913 - val_accuracy: 0.9179\n",
            "Epoch 36/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0276 - accuracy: 0.9916 - val_loss: 0.4158 - val_accuracy: 0.9199\n",
            "Epoch 37/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0190 - accuracy: 0.9936 - val_loss: 0.4782 - val_accuracy: 0.9054\n",
            "Epoch 38/150\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 0.0301 - accuracy: 0.9886 - val_loss: 0.4571 - val_accuracy: 0.9059\n",
            "Epoch 39/150\n",
            "250/250 [==============================] - 2s 6ms/step - loss: 0.0509 - accuracy: 0.9859 - val_loss: 0.4478 - val_accuracy: 0.9024\n",
            "Epoch 40/150\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 0.0171 - accuracy: 0.9931 - val_loss: 0.4376 - val_accuracy: 0.9149\n",
            "Epoch 41/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0191 - accuracy: 0.9955 - val_loss: 0.3908 - val_accuracy: 0.9249\n",
            "Epoch 42/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0386 - accuracy: 0.9890 - val_loss: 0.4390 - val_accuracy: 0.9074\n",
            "Epoch 43/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0308 - accuracy: 0.9921 - val_loss: 0.4461 - val_accuracy: 0.9049\n",
            "Epoch 44/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0181 - accuracy: 0.9942 - val_loss: 0.4554 - val_accuracy: 0.9134\n",
            "Epoch 45/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0157 - accuracy: 0.9947 - val_loss: 0.6126 - val_accuracy: 0.8934\n",
            "Epoch 46/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0493 - accuracy: 0.9856 - val_loss: 0.5266 - val_accuracy: 0.9044\n",
            "Epoch 47/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0341 - accuracy: 0.9899 - val_loss: 0.4367 - val_accuracy: 0.9124\n",
            "Epoch 48/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0296 - accuracy: 0.9901 - val_loss: 0.5175 - val_accuracy: 0.8994\n",
            "Epoch 49/150\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 0.0198 - accuracy: 0.9925 - val_loss: 0.4409 - val_accuracy: 0.9189\n",
            "Epoch 50/150\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 0.0091 - accuracy: 0.9969 - val_loss: 0.5368 - val_accuracy: 0.9184\n",
            "Epoch 51/150\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 0.0429 - accuracy: 0.9895 - val_loss: 0.5182 - val_accuracy: 0.9094\n",
            "Epoch 52/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0264 - accuracy: 0.9920 - val_loss: 0.4868 - val_accuracy: 0.9124\n",
            "Epoch 53/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0320 - accuracy: 0.9914 - val_loss: 0.5352 - val_accuracy: 0.9099\n",
            "Epoch 54/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0148 - accuracy: 0.9951 - val_loss: 0.4531 - val_accuracy: 0.9139\n",
            "Epoch 55/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0238 - accuracy: 0.9931 - val_loss: 0.3930 - val_accuracy: 0.9244\n",
            "Epoch 56/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0071 - accuracy: 0.9972 - val_loss: 0.4185 - val_accuracy: 0.9239\n",
            "Epoch 57/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0024 - accuracy: 0.9990 - val_loss: 0.4183 - val_accuracy: 0.9264\n",
            "Epoch 58/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0025 - accuracy: 0.9986 - val_loss: 0.4137 - val_accuracy: 0.9249\n",
            "Epoch 59/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0014 - accuracy: 0.9992 - val_loss: 0.4164 - val_accuracy: 0.9264\n",
            "Epoch 60/150\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 0.0015 - accuracy: 0.9987 - val_loss: 0.4226 - val_accuracy: 0.9274\n",
            "Epoch 61/150\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 0.0018 - accuracy: 0.9989 - val_loss: 0.4283 - val_accuracy: 0.9279\n",
            "Epoch 62/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.1071 - accuracy: 0.9690 - val_loss: 0.4567 - val_accuracy: 0.8924\n",
            "Epoch 63/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0433 - accuracy: 0.9849 - val_loss: 0.4501 - val_accuracy: 0.9164\n",
            "Epoch 64/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0242 - accuracy: 0.9919 - val_loss: 0.5498 - val_accuracy: 0.9089\n",
            "Epoch 65/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0255 - accuracy: 0.9926 - val_loss: 0.4122 - val_accuracy: 0.9204\n",
            "Epoch 66/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0031 - accuracy: 0.9989 - val_loss: 0.3886 - val_accuracy: 0.9254\n",
            "Epoch 67/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0014 - accuracy: 0.9994 - val_loss: 0.3895 - val_accuracy: 0.9264\n",
            "Epoch 68/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0014 - accuracy: 0.9987 - val_loss: 0.3929 - val_accuracy: 0.9269\n",
            "Epoch 69/150\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 0.0013 - accuracy: 0.9992 - val_loss: 0.3976 - val_accuracy: 0.9274\n",
            "Epoch 70/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0014 - accuracy: 0.9995 - val_loss: 0.3986 - val_accuracy: 0.9274\n",
            "Epoch 71/150\n",
            "250/250 [==============================] - 2s 6ms/step - loss: 0.0013 - accuracy: 0.9992 - val_loss: 0.4040 - val_accuracy: 0.9259\n",
            "Epoch 72/150\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 0.0013 - accuracy: 0.9990 - val_loss: 0.4065 - val_accuracy: 0.9259\n",
            "Epoch 73/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0013 - accuracy: 0.9989 - val_loss: 0.4102 - val_accuracy: 0.9264\n",
            "Epoch 74/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0013 - accuracy: 0.9991 - val_loss: 0.4153 - val_accuracy: 0.9249\n",
            "Epoch 75/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0013 - accuracy: 0.9990 - val_loss: 0.4204 - val_accuracy: 0.9264\n",
            "Epoch 76/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0013 - accuracy: 0.9990 - val_loss: 0.4217 - val_accuracy: 0.9264\n",
            "Epoch 77/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0013 - accuracy: 0.9986 - val_loss: 0.4211 - val_accuracy: 0.9249\n",
            "Epoch 78/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0012 - accuracy: 0.9992 - val_loss: 0.4300 - val_accuracy: 0.9254\n",
            "Epoch 79/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0013 - accuracy: 0.9989 - val_loss: 0.4370 - val_accuracy: 0.9259\n",
            "Epoch 80/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0012 - accuracy: 0.9991 - val_loss: 0.4402 - val_accuracy: 0.9244\n",
            "Epoch 81/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0013 - accuracy: 0.9992 - val_loss: 0.4500 - val_accuracy: 0.9254\n",
            "Epoch 82/150\n",
            "250/250 [==============================] - 2s 6ms/step - loss: 0.0013 - accuracy: 0.9989 - val_loss: 0.4413 - val_accuracy: 0.9259\n",
            "Epoch 83/150\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 0.1634 - accuracy: 0.9565 - val_loss: 0.5065 - val_accuracy: 0.8694\n",
            "Epoch 84/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0951 - accuracy: 0.9697 - val_loss: 0.4643 - val_accuracy: 0.9039\n",
            "Epoch 85/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0197 - accuracy: 0.9939 - val_loss: 0.4131 - val_accuracy: 0.9189\n",
            "Epoch 86/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0200 - accuracy: 0.9955 - val_loss: 0.4362 - val_accuracy: 0.9184\n",
            "Epoch 87/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0089 - accuracy: 0.9979 - val_loss: 0.4411 - val_accuracy: 0.9234\n",
            "Epoch 88/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0020 - accuracy: 0.9991 - val_loss: 0.4356 - val_accuracy: 0.9254\n",
            "Epoch 89/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0014 - accuracy: 0.9989 - val_loss: 0.4481 - val_accuracy: 0.9274\n",
            "Epoch 90/150\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 0.0013 - accuracy: 0.9991 - val_loss: 0.4544 - val_accuracy: 0.9284\n",
            "Epoch 91/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0013 - accuracy: 0.9989 - val_loss: 0.4626 - val_accuracy: 0.9284\n",
            "Epoch 92/150\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 0.0013 - accuracy: 0.9991 - val_loss: 0.4685 - val_accuracy: 0.9284\n",
            "Epoch 93/150\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 0.0014 - accuracy: 0.9987 - val_loss: 0.4761 - val_accuracy: 0.9279\n",
            "Epoch 94/150\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 0.0013 - accuracy: 0.9990 - val_loss: 0.4827 - val_accuracy: 0.9299\n",
            "Epoch 95/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0040 - accuracy: 0.9985 - val_loss: 0.5765 - val_accuracy: 0.9234\n",
            "Epoch 96/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.1256 - accuracy: 0.9653 - val_loss: 0.5443 - val_accuracy: 0.8979\n",
            "Epoch 97/150\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 0.0555 - accuracy: 0.9857 - val_loss: 0.5429 - val_accuracy: 0.9109\n",
            "Epoch 98/150\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 0.0196 - accuracy: 0.9940 - val_loss: 0.5526 - val_accuracy: 0.9079\n",
            "Epoch 99/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0099 - accuracy: 0.9966 - val_loss: 0.5160 - val_accuracy: 0.9209\n",
            "Epoch 100/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0066 - accuracy: 0.9974 - val_loss: 0.6334 - val_accuracy: 0.9089\n",
            "Epoch 101/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0162 - accuracy: 0.9954 - val_loss: 0.5242 - val_accuracy: 0.9139\n",
            "Epoch 102/150\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 0.0306 - accuracy: 0.9907 - val_loss: 0.5848 - val_accuracy: 0.9044\n",
            "Epoch 103/150\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 0.0418 - accuracy: 0.9887 - val_loss: 0.4225 - val_accuracy: 0.9159\n",
            "Epoch 104/150\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 0.0149 - accuracy: 0.9954 - val_loss: 0.4875 - val_accuracy: 0.9189\n",
            "Epoch 105/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0108 - accuracy: 0.9971 - val_loss: 0.4296 - val_accuracy: 0.9239\n",
            "Epoch 106/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0075 - accuracy: 0.9972 - val_loss: 0.4717 - val_accuracy: 0.9214\n",
            "Epoch 107/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0031 - accuracy: 0.9986 - val_loss: 0.4020 - val_accuracy: 0.9279\n",
            "Epoch 108/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0025 - accuracy: 0.9986 - val_loss: 0.4251 - val_accuracy: 0.9269\n",
            "Epoch 109/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0013 - accuracy: 0.9990 - val_loss: 0.4291 - val_accuracy: 0.9264\n",
            "Epoch 110/150\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 0.0015 - accuracy: 0.9986 - val_loss: 0.4316 - val_accuracy: 0.9294\n",
            "Epoch 111/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0173 - accuracy: 0.9955 - val_loss: 0.7096 - val_accuracy: 0.8899\n",
            "Epoch 112/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0450 - accuracy: 0.9865 - val_loss: 0.6013 - val_accuracy: 0.9034\n",
            "Epoch 113/150\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 0.0296 - accuracy: 0.9910 - val_loss: 0.4887 - val_accuracy: 0.9169\n",
            "Epoch 114/150\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 0.0102 - accuracy: 0.9965 - val_loss: 0.4760 - val_accuracy: 0.9194\n",
            "Epoch 115/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0339 - accuracy: 0.9911 - val_loss: 0.4769 - val_accuracy: 0.9219\n",
            "Epoch 116/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0198 - accuracy: 0.9941 - val_loss: 0.4810 - val_accuracy: 0.9224\n",
            "Epoch 117/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0108 - accuracy: 0.9961 - val_loss: 0.4801 - val_accuracy: 0.9234\n",
            "Epoch 118/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0049 - accuracy: 0.9981 - val_loss: 0.4866 - val_accuracy: 0.9234\n",
            "Epoch 119/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0015 - accuracy: 0.9989 - val_loss: 0.4786 - val_accuracy: 0.9249\n",
            "Epoch 120/150\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 0.0013 - accuracy: 0.9989 - val_loss: 0.4821 - val_accuracy: 0.9244\n",
            "Epoch 121/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0012 - accuracy: 0.9990 - val_loss: 0.4844 - val_accuracy: 0.9249\n",
            "Epoch 122/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0013 - accuracy: 0.9990 - val_loss: 0.4882 - val_accuracy: 0.9264\n",
            "Epoch 123/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0012 - accuracy: 0.9990 - val_loss: 0.4904 - val_accuracy: 0.9264\n",
            "Epoch 124/150\n",
            "250/250 [==============================] - 2s 6ms/step - loss: 0.0012 - accuracy: 0.9992 - val_loss: 0.4926 - val_accuracy: 0.9274\n",
            "Epoch 125/150\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 0.0012 - accuracy: 0.9991 - val_loss: 0.4962 - val_accuracy: 0.9279\n",
            "Epoch 126/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0012 - accuracy: 0.9990 - val_loss: 0.5002 - val_accuracy: 0.9294\n",
            "Epoch 127/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0267 - accuracy: 0.9940 - val_loss: 0.6877 - val_accuracy: 0.8854\n",
            "Epoch 128/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.1297 - accuracy: 0.9650 - val_loss: 0.4480 - val_accuracy: 0.9104\n",
            "Epoch 129/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0217 - accuracy: 0.9929 - val_loss: 0.3877 - val_accuracy: 0.9219\n",
            "Epoch 130/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0044 - accuracy: 0.9980 - val_loss: 0.3699 - val_accuracy: 0.9304\n",
            "Epoch 131/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0018 - accuracy: 0.9987 - val_loss: 0.3758 - val_accuracy: 0.9304\n",
            "Epoch 132/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0014 - accuracy: 0.9990 - val_loss: 0.3780 - val_accuracy: 0.9309\n",
            "Epoch 133/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0013 - accuracy: 0.9994 - val_loss: 0.3835 - val_accuracy: 0.9319\n",
            "Epoch 134/150\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 0.0013 - accuracy: 0.9989 - val_loss: 0.3875 - val_accuracy: 0.9314\n",
            "Epoch 135/150\n",
            "250/250 [==============================] - 2s 6ms/step - loss: 0.0013 - accuracy: 0.9989 - val_loss: 0.3912 - val_accuracy: 0.9324\n",
            "Epoch 136/150\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 0.0012 - accuracy: 0.9990 - val_loss: 0.3947 - val_accuracy: 0.9324\n",
            "Epoch 137/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 9.6472e-04 - accuracy: 0.9996 - val_loss: 0.4078 - val_accuracy: 0.9334\n",
            "Epoch 138/150\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 0.0018 - accuracy: 0.9990 - val_loss: 0.3992 - val_accuracy: 0.9354\n",
            "Epoch 139/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0701 - accuracy: 0.9816 - val_loss: 0.6046 - val_accuracy: 0.8909\n",
            "Epoch 140/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0423 - accuracy: 0.9877 - val_loss: 0.6025 - val_accuracy: 0.9174\n",
            "Epoch 141/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0197 - accuracy: 0.9941 - val_loss: 0.5467 - val_accuracy: 0.9194\n",
            "Epoch 142/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0092 - accuracy: 0.9969 - val_loss: 0.5302 - val_accuracy: 0.9244\n",
            "Epoch 143/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0148 - accuracy: 0.9960 - val_loss: 0.5855 - val_accuracy: 0.9164\n",
            "Epoch 144/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0257 - accuracy: 0.9925 - val_loss: 0.5174 - val_accuracy: 0.9244\n",
            "Epoch 145/150\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 0.0069 - accuracy: 0.9981 - val_loss: 0.5272 - val_accuracy: 0.9239\n",
            "Epoch 146/150\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 0.0037 - accuracy: 0.9982 - val_loss: 0.5224 - val_accuracy: 0.9294\n",
            "Epoch 147/150\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 0.0053 - accuracy: 0.9981 - val_loss: 0.4767 - val_accuracy: 0.9299\n",
            "Epoch 148/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0049 - accuracy: 0.9982 - val_loss: 0.4928 - val_accuracy: 0.9294\n",
            "Epoch 149/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0014 - accuracy: 0.9989 - val_loss: 0.4849 - val_accuracy: 0.9299\n",
            "Epoch 150/150\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 0.0013 - accuracy: 0.9990 - val_loss: 0.4955 - val_accuracy: 0.9309\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "dp_model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(512, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax'),\n",
        "])\n",
        "\n",
        "print(dp_model.summary())\n",
        "\n",
        "dp_model.compile(optimizer='adam',\n",
        "                 loss='sparse_categorical_crossentropy',\n",
        "                 metrics='accuracy')\n",
        "\n",
        "history = dp_model.fit(X_train,\n",
        "                    y_train,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    epochs=150,\n",
        "                    batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeOYOI7II95S"
      },
      "source": [
        "#### Score evaluation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58ysic2wzjQs",
        "outputId": "435234e4-f479-4a59-875b-0258524e6320"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 0s 2ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       blues       0.94      0.93      0.93       208\n",
            "   classical       0.95      0.98      0.96       203\n",
            "     country       0.87      0.90      0.89       186\n",
            "       disco       0.92      0.92      0.92       199\n",
            "      hiphop       0.95      0.94      0.95       218\n",
            "        jazz       0.91      0.92      0.91       192\n",
            "       metal       0.98      0.99      0.98       204\n",
            "         pop       0.95      0.96      0.96       180\n",
            "      reggae       0.92      0.93      0.93       211\n",
            "        rock       0.90      0.85      0.87       197\n",
            "\n",
            "    accuracy                           0.93      1998\n",
            "   macro avg       0.93      0.93      0.93      1998\n",
            "weighted avg       0.93      0.93      0.93      1998\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_val_predict = np.argmax(dp_model.predict(X_val), axis=1)\n",
        "\n",
        "# Evaludation\n",
        "report = classification_report(y_val, y_val_predict, target_names=encoder.classes_)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqsTKUgRJkkL"
      },
      "source": [
        "### Test DP model with different dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idHBuVdjJk_M",
        "outputId": "95443d07-0e84-4dc7-d40e-ba81803c582f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "# Dropout: 0.1\n",
            "\n",
            "63/63 [==============================] - 0s 3ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       blues       0.93      0.93      0.93       208\n",
            "   classical       0.97      0.93      0.95       203\n",
            "     country       0.89      0.90      0.90       186\n",
            "       disco       0.92      0.90      0.91       199\n",
            "      hiphop       0.92      0.95      0.94       218\n",
            "        jazz       0.88      0.97      0.92       192\n",
            "       metal       0.95      0.98      0.96       204\n",
            "         pop       0.94      0.95      0.95       180\n",
            "      reggae       0.92      0.91      0.92       211\n",
            "        rock       0.94      0.83      0.88       197\n",
            "\n",
            "    accuracy                           0.93      1998\n",
            "   macro avg       0.93      0.93      0.93      1998\n",
            "weighted avg       0.93      0.93      0.93      1998\n",
            "\n",
            "\n",
            "\n",
            "# Dropout: 0.2\n",
            "\n",
            "63/63 [==============================] - 0s 2ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       blues       0.94      0.92      0.93       208\n",
            "   classical       0.94      0.95      0.94       203\n",
            "     country       0.82      0.91      0.86       186\n",
            "       disco       0.92      0.92      0.92       199\n",
            "      hiphop       0.98      0.92      0.95       218\n",
            "        jazz       0.90      0.93      0.92       192\n",
            "       metal       0.96      0.97      0.96       204\n",
            "         pop       0.90      0.94      0.92       180\n",
            "      reggae       0.95      0.89      0.92       211\n",
            "        rock       0.90      0.87      0.88       197\n",
            "\n",
            "    accuracy                           0.92      1998\n",
            "   macro avg       0.92      0.92      0.92      1998\n",
            "weighted avg       0.92      0.92      0.92      1998\n",
            "\n",
            "\n",
            "\n",
            "# Dropout: 0.30000000000000004\n",
            "\n",
            "63/63 [==============================] - 0s 2ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       blues       0.91      0.92      0.92       208\n",
            "   classical       0.92      0.97      0.94       203\n",
            "     country       0.82      0.87      0.85       186\n",
            "       disco       0.91      0.87      0.89       199\n",
            "      hiphop       0.92      0.95      0.93       218\n",
            "        jazz       0.91      0.88      0.89       192\n",
            "       metal       0.97      0.96      0.96       204\n",
            "         pop       0.93      0.93      0.93       180\n",
            "      reggae       0.92      0.90      0.91       211\n",
            "        rock       0.87      0.83      0.85       197\n",
            "\n",
            "    accuracy                           0.91      1998\n",
            "   macro avg       0.91      0.91      0.91      1998\n",
            "weighted avg       0.91      0.91      0.91      1998\n",
            "\n",
            "\n",
            "\n",
            "# Dropout: 0.4\n",
            "\n",
            "63/63 [==============================] - 0s 2ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       blues       0.95      0.88      0.92       208\n",
            "   classical       0.91      0.97      0.94       203\n",
            "     country       0.88      0.90      0.89       186\n",
            "       disco       0.90      0.92      0.91       199\n",
            "      hiphop       0.97      0.94      0.95       218\n",
            "        jazz       0.91      0.93      0.92       192\n",
            "       metal       0.96      0.98      0.97       204\n",
            "         pop       0.95      0.94      0.95       180\n",
            "      reggae       0.95      0.93      0.94       211\n",
            "        rock       0.88      0.85      0.86       197\n",
            "\n",
            "    accuracy                           0.93      1998\n",
            "   macro avg       0.93      0.93      0.92      1998\n",
            "weighted avg       0.93      0.93      0.93      1998\n",
            "\n",
            "\n",
            "\n",
            "# Dropout: 0.5\n",
            "\n",
            "63/63 [==============================] - 0s 2ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       blues       0.96      0.93      0.95       208\n",
            "   classical       0.94      0.94      0.94       203\n",
            "     country       0.86      0.92      0.89       186\n",
            "       disco       0.92      0.92      0.92       199\n",
            "      hiphop       0.96      0.93      0.94       218\n",
            "        jazz       0.87      0.95      0.91       192\n",
            "       metal       0.98      0.96      0.97       204\n",
            "         pop       0.97      0.93      0.95       180\n",
            "      reggae       0.94      0.95      0.94       211\n",
            "        rock       0.92      0.88      0.90       197\n",
            "\n",
            "    accuracy                           0.93      1998\n",
            "   macro avg       0.93      0.93      0.93      1998\n",
            "weighted avg       0.93      0.93      0.93      1998\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def create_dp_model_with_dropout(dropout):\n",
        "    dp_model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Dense(512, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "        tf.keras.layers.Dropout(dropout),\n",
        "\n",
        "        tf.keras.layers.Dense(256, activation='relu'),\n",
        "        tf.keras.layers.Dropout(dropout),\n",
        "\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dropout(dropout),\n",
        "\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dropout(dropout),\n",
        "\n",
        "        tf.keras.layers.Dense(10, activation='softmax'),\n",
        "    ])\n",
        "\n",
        "    dp_model.compile(optimizer='adam',\n",
        "                    loss='sparse_categorical_crossentropy',\n",
        "                    metrics='accuracy')\n",
        "\n",
        "    return dp_model\n",
        "\n",
        "for i in range(1, 6):\n",
        "    dropout = i * 0.1\n",
        "    print(f\"\\n\\n# Dropout: {dropout}\\n\")\n",
        "    model = create_dp_model_with_dropout(dropout)\n",
        "    dp_model.fit(X_train,\n",
        "                    y_train,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    epochs=150,\n",
        "                    batch_size=32,\n",
        "                    verbose=None)\n",
        "\n",
        "    y_val_predict = np.argmax(dp_model.predict(X_val), axis=1)\n",
        "\n",
        "    # Evaludation\n",
        "    report = classification_report(y_val, y_val_predict, target_names=encoder.classes_)\n",
        "    print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygSeCgTW3luS"
      },
      "source": [
        "## Test larger DP model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euBIveCO3mNW",
        "outputId": "7116a101-7d8c-496e-e6e6-32385af620ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_10 (Dense)            (None, 1024)              60416     \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 512)               524800    \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 256)               131328    \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 128)               32896     \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 758346 (2.89 MB)\n",
            "Trainable params: 758346 (2.89 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/150\n",
            "250/250 [==============================] - 10s 28ms/step - loss: 1.3725 - accuracy: 0.5143 - val_loss: 0.9145 - val_accuracy: 0.6767\n",
            "Epoch 2/150\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.9127 - accuracy: 0.6947 - val_loss: 0.7280 - val_accuracy: 0.7457\n",
            "Epoch 3/150\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 0.7263 - accuracy: 0.7660 - val_loss: 0.6242 - val_accuracy: 0.7913\n",
            "Epoch 4/150\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 0.6061 - accuracy: 0.8001 - val_loss: 0.5692 - val_accuracy: 0.8273\n",
            "Epoch 5/150\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.5080 - accuracy: 0.8350 - val_loss: 0.5252 - val_accuracy: 0.8438\n",
            "Epoch 6/150\n",
            "250/250 [==============================] - 4s 18ms/step - loss: 0.4384 - accuracy: 0.8592 - val_loss: 0.4926 - val_accuracy: 0.8569\n",
            "Epoch 7/150\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.3983 - accuracy: 0.8741 - val_loss: 0.4317 - val_accuracy: 0.8704\n",
            "Epoch 8/150\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 0.3536 - accuracy: 0.8934 - val_loss: 0.4092 - val_accuracy: 0.8794\n",
            "Epoch 9/150\n",
            "250/250 [==============================] - 4s 18ms/step - loss: 0.3036 - accuracy: 0.9055 - val_loss: 0.4244 - val_accuracy: 0.8709\n",
            "Epoch 10/150\n",
            "250/250 [==============================] - 8s 32ms/step - loss: 0.2740 - accuracy: 0.9129 - val_loss: 0.4128 - val_accuracy: 0.8784\n",
            "Epoch 11/150\n",
            "250/250 [==============================] - 7s 27ms/step - loss: 0.2492 - accuracy: 0.9218 - val_loss: 0.3538 - val_accuracy: 0.8884\n",
            "Epoch 12/150\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 0.2394 - accuracy: 0.9276 - val_loss: 0.3906 - val_accuracy: 0.8919\n",
            "Epoch 13/150\n",
            "250/250 [==============================] - 4s 15ms/step - loss: 0.1983 - accuracy: 0.9366 - val_loss: 0.4347 - val_accuracy: 0.8754\n",
            "Epoch 14/150\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 0.1978 - accuracy: 0.9363 - val_loss: 0.3482 - val_accuracy: 0.8984\n",
            "Epoch 15/150\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 0.1968 - accuracy: 0.9411 - val_loss: 0.4000 - val_accuracy: 0.8899\n",
            "Epoch 16/150\n",
            "250/250 [==============================] - 4s 15ms/step - loss: 0.1660 - accuracy: 0.9477 - val_loss: 0.3489 - val_accuracy: 0.8994\n",
            "Epoch 17/150\n",
            "250/250 [==============================] - 5s 20ms/step - loss: 0.1465 - accuracy: 0.9580 - val_loss: 0.3144 - val_accuracy: 0.9149\n",
            "Epoch 18/150\n",
            "250/250 [==============================] - 5s 18ms/step - loss: 0.1551 - accuracy: 0.9501 - val_loss: 0.3098 - val_accuracy: 0.9159\n",
            "Epoch 19/150\n",
            "250/250 [==============================] - 4s 18ms/step - loss: 0.1565 - accuracy: 0.9545 - val_loss: 0.3563 - val_accuracy: 0.9064\n",
            "Epoch 20/150\n",
            "250/250 [==============================] - 5s 20ms/step - loss: 0.1314 - accuracy: 0.9595 - val_loss: 0.3631 - val_accuracy: 0.9119\n",
            "Epoch 21/150\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 0.1234 - accuracy: 0.9610 - val_loss: 0.3689 - val_accuracy: 0.9099\n",
            "Epoch 22/150\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 0.1262 - accuracy: 0.9627 - val_loss: 0.3606 - val_accuracy: 0.9069\n",
            "Epoch 23/150\n",
            "250/250 [==============================] - 5s 20ms/step - loss: 0.1263 - accuracy: 0.9628 - val_loss: 0.3388 - val_accuracy: 0.9119\n",
            "Epoch 24/150\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 0.0985 - accuracy: 0.9690 - val_loss: 0.3342 - val_accuracy: 0.9159\n",
            "Epoch 25/150\n",
            "250/250 [==============================] - 4s 15ms/step - loss: 0.0997 - accuracy: 0.9703 - val_loss: 0.3793 - val_accuracy: 0.9119\n",
            "Epoch 26/150\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 0.1202 - accuracy: 0.9642 - val_loss: 0.3776 - val_accuracy: 0.9059\n",
            "Epoch 27/150\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 0.1133 - accuracy: 0.9665 - val_loss: 0.3568 - val_accuracy: 0.9144\n",
            "Epoch 28/150\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 0.0871 - accuracy: 0.9754 - val_loss: 0.3998 - val_accuracy: 0.9144\n",
            "Epoch 29/150\n",
            "250/250 [==============================] - 5s 20ms/step - loss: 0.0972 - accuracy: 0.9706 - val_loss: 0.4336 - val_accuracy: 0.8984\n",
            "Epoch 30/150\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 0.0969 - accuracy: 0.9731 - val_loss: 0.3926 - val_accuracy: 0.9119\n",
            "Epoch 31/150\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 0.0904 - accuracy: 0.9731 - val_loss: 0.3328 - val_accuracy: 0.9299\n",
            "Epoch 32/150\n",
            "250/250 [==============================] - 5s 20ms/step - loss: 0.0837 - accuracy: 0.9751 - val_loss: 0.3716 - val_accuracy: 0.9249\n",
            "Epoch 33/150\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 0.0731 - accuracy: 0.9766 - val_loss: 0.4464 - val_accuracy: 0.9159\n",
            "Epoch 34/150\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 0.0876 - accuracy: 0.9751 - val_loss: 0.3727 - val_accuracy: 0.9199\n",
            "Epoch 35/150\n",
            "250/250 [==============================] - 5s 20ms/step - loss: 0.0817 - accuracy: 0.9766 - val_loss: 0.3971 - val_accuracy: 0.9139\n",
            "Epoch 36/150\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 0.0768 - accuracy: 0.9766 - val_loss: 0.3628 - val_accuracy: 0.9254\n",
            "Epoch 37/150\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.0878 - accuracy: 0.9752 - val_loss: 0.3674 - val_accuracy: 0.9219\n",
            "Epoch 38/150\n",
            "250/250 [==============================] - 5s 20ms/step - loss: 0.0698 - accuracy: 0.9776 - val_loss: 0.4046 - val_accuracy: 0.9239\n",
            "Epoch 39/150\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 0.0705 - accuracy: 0.9805 - val_loss: 0.3960 - val_accuracy: 0.9229\n",
            "Epoch 40/150\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 0.0684 - accuracy: 0.9784 - val_loss: 0.4054 - val_accuracy: 0.9259\n",
            "Epoch 41/150\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 0.0762 - accuracy: 0.9774 - val_loss: 0.3170 - val_accuracy: 0.9314\n",
            "Epoch 42/150\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 0.0813 - accuracy: 0.9772 - val_loss: 0.3774 - val_accuracy: 0.9234\n",
            "Epoch 43/150\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 0.0899 - accuracy: 0.9728 - val_loss: 0.3970 - val_accuracy: 0.9184\n",
            "Epoch 44/150\n",
            "250/250 [==============================] - 5s 21ms/step - loss: 0.0769 - accuracy: 0.9782 - val_loss: 0.3748 - val_accuracy: 0.9159\n",
            "Epoch 45/150\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 0.0601 - accuracy: 0.9830 - val_loss: 0.4308 - val_accuracy: 0.9139\n",
            "Epoch 46/150\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 0.0550 - accuracy: 0.9842 - val_loss: 0.3854 - val_accuracy: 0.9279\n",
            "Epoch 47/150\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 0.0770 - accuracy: 0.9784 - val_loss: 0.4644 - val_accuracy: 0.9154\n",
            "Epoch 48/150\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 0.0637 - accuracy: 0.9817 - val_loss: 0.3816 - val_accuracy: 0.9274\n",
            "Epoch 49/150\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 0.0593 - accuracy: 0.9831 - val_loss: 0.4803 - val_accuracy: 0.9194\n",
            "Epoch 50/150\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 0.0866 - accuracy: 0.9743 - val_loss: 0.3311 - val_accuracy: 0.9274\n",
            "Epoch 51/150\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 0.0468 - accuracy: 0.9854 - val_loss: 0.4783 - val_accuracy: 0.9254\n",
            "Epoch 52/150\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 0.0618 - accuracy: 0.9820 - val_loss: 0.3904 - val_accuracy: 0.9329\n",
            "Epoch 53/150\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 0.0690 - accuracy: 0.9802 - val_loss: 0.3403 - val_accuracy: 0.9294\n",
            "Epoch 54/150\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.0547 - accuracy: 0.9830 - val_loss: 0.3362 - val_accuracy: 0.9369\n",
            "Epoch 55/150\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 0.0582 - accuracy: 0.9831 - val_loss: 0.3384 - val_accuracy: 0.9394\n",
            "Epoch 56/150\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 0.0482 - accuracy: 0.9875 - val_loss: 0.4000 - val_accuracy: 0.9354\n",
            "Epoch 57/150\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.0626 - accuracy: 0.9812 - val_loss: 0.3774 - val_accuracy: 0.9299\n",
            "Epoch 58/150\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 0.0639 - accuracy: 0.9806 - val_loss: 0.4074 - val_accuracy: 0.9294\n",
            "Epoch 59/150\n",
            "250/250 [==============================] - 4s 18ms/step - loss: 0.0614 - accuracy: 0.9840 - val_loss: 0.3888 - val_accuracy: 0.9289\n",
            "Epoch 60/150\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.0509 - accuracy: 0.9869 - val_loss: 0.3295 - val_accuracy: 0.9369\n",
            "Epoch 61/150\n",
            "250/250 [==============================] - 5s 20ms/step - loss: 0.0416 - accuracy: 0.9890 - val_loss: 0.4241 - val_accuracy: 0.9249\n",
            "Epoch 62/150\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 0.0393 - accuracy: 0.9874 - val_loss: 0.4790 - val_accuracy: 0.9234\n",
            "Epoch 63/150\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.0783 - accuracy: 0.9787 - val_loss: 0.4139 - val_accuracy: 0.9264\n",
            "Epoch 64/150\n",
            "250/250 [==============================] - 5s 20ms/step - loss: 0.0427 - accuracy: 0.9887 - val_loss: 0.4752 - val_accuracy: 0.9269\n",
            "Epoch 65/150\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 0.0724 - accuracy: 0.9801 - val_loss: 0.4138 - val_accuracy: 0.9274\n",
            "Epoch 66/150\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 0.0691 - accuracy: 0.9826 - val_loss: 0.3827 - val_accuracy: 0.9189\n",
            "Epoch 67/150\n",
            "250/250 [==============================] - 5s 20ms/step - loss: 0.0604 - accuracy: 0.9824 - val_loss: 0.4303 - val_accuracy: 0.9284\n",
            "Epoch 68/150\n",
            "250/250 [==============================] - 5s 18ms/step - loss: 0.0422 - accuracy: 0.9890 - val_loss: 0.4885 - val_accuracy: 0.9289\n",
            "Epoch 69/150\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.0429 - accuracy: 0.9886 - val_loss: 0.4475 - val_accuracy: 0.9244\n",
            "Epoch 70/150\n",
            "250/250 [==============================] - 5s 20ms/step - loss: 0.0366 - accuracy: 0.9887 - val_loss: 0.4038 - val_accuracy: 0.9274\n",
            "Epoch 71/150\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.0663 - accuracy: 0.9842 - val_loss: 0.4134 - val_accuracy: 0.9259\n",
            "Epoch 72/150\n",
            "250/250 [==============================] - 4s 18ms/step - loss: 0.0442 - accuracy: 0.9869 - val_loss: 0.4246 - val_accuracy: 0.9284\n",
            "Epoch 73/150\n",
            "250/250 [==============================] - 5s 18ms/step - loss: 0.0429 - accuracy: 0.9877 - val_loss: 0.5249 - val_accuracy: 0.9274\n",
            "Epoch 74/150\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.0436 - accuracy: 0.9876 - val_loss: 0.4400 - val_accuracy: 0.9309\n",
            "Epoch 75/150\n",
            "250/250 [==============================] - 4s 18ms/step - loss: 0.0438 - accuracy: 0.9875 - val_loss: 0.4772 - val_accuracy: 0.9274\n",
            "Epoch 76/150\n",
            "250/250 [==============================] - 5s 18ms/step - loss: 0.0801 - accuracy: 0.9805 - val_loss: 0.4469 - val_accuracy: 0.9234\n",
            "Epoch 77/150\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.0568 - accuracy: 0.9837 - val_loss: 0.3375 - val_accuracy: 0.9354\n",
            "Epoch 78/150\n",
            "250/250 [==============================] - 5s 21ms/step - loss: 0.0387 - accuracy: 0.9880 - val_loss: 0.4348 - val_accuracy: 0.9404\n",
            "Epoch 79/150\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.0535 - accuracy: 0.9875 - val_loss: 0.4396 - val_accuracy: 0.9284\n",
            "Epoch 80/150\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 0.0491 - accuracy: 0.9875 - val_loss: 0.3489 - val_accuracy: 0.9364\n",
            "Epoch 81/150\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 0.0348 - accuracy: 0.9891 - val_loss: 0.4501 - val_accuracy: 0.9264\n",
            "Epoch 82/150\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 0.0541 - accuracy: 0.9862 - val_loss: 0.3980 - val_accuracy: 0.9359\n",
            "Epoch 83/150\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 0.0500 - accuracy: 0.9865 - val_loss: 0.4456 - val_accuracy: 0.9309\n",
            "Epoch 84/150\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 0.0597 - accuracy: 0.9847 - val_loss: 0.3618 - val_accuracy: 0.9289\n",
            "Epoch 85/150\n",
            "250/250 [==============================] - 7s 27ms/step - loss: 0.0420 - accuracy: 0.9866 - val_loss: 0.3945 - val_accuracy: 0.9444\n",
            "Epoch 86/150\n",
            "250/250 [==============================] - 5s 18ms/step - loss: 0.0427 - accuracy: 0.9889 - val_loss: 0.4043 - val_accuracy: 0.9389\n",
            "Epoch 87/150\n",
            "250/250 [==============================] - 5s 18ms/step - loss: 0.0389 - accuracy: 0.9880 - val_loss: 0.3711 - val_accuracy: 0.9394\n",
            "Epoch 88/150\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 0.0430 - accuracy: 0.9872 - val_loss: 0.4239 - val_accuracy: 0.9344\n",
            "Epoch 89/150\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 0.0464 - accuracy: 0.9885 - val_loss: 0.3965 - val_accuracy: 0.9364\n",
            "Epoch 90/150\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.0487 - accuracy: 0.9867 - val_loss: 0.3445 - val_accuracy: 0.9369\n",
            "Epoch 91/150\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 0.0363 - accuracy: 0.9899 - val_loss: 0.4896 - val_accuracy: 0.9259\n",
            "Epoch 92/150\n",
            "250/250 [==============================] - 5s 20ms/step - loss: 0.0421 - accuracy: 0.9882 - val_loss: 0.3802 - val_accuracy: 0.9444\n",
            "Epoch 93/150\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.0540 - accuracy: 0.9860 - val_loss: 0.4086 - val_accuracy: 0.9339\n",
            "Epoch 94/150\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 0.0700 - accuracy: 0.9839 - val_loss: 0.3308 - val_accuracy: 0.9269\n",
            "Epoch 95/150\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 0.0710 - accuracy: 0.9811 - val_loss: 0.3371 - val_accuracy: 0.9374\n",
            "Epoch 96/150\n",
            "250/250 [==============================] - 5s 22ms/step - loss: 0.0268 - accuracy: 0.9931 - val_loss: 0.4296 - val_accuracy: 0.9359\n",
            "Epoch 97/150\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.0630 - accuracy: 0.9852 - val_loss: 0.3843 - val_accuracy: 0.9404\n",
            "Epoch 98/150\n",
            "250/250 [==============================] - 5s 20ms/step - loss: 0.0442 - accuracy: 0.9895 - val_loss: 0.4491 - val_accuracy: 0.9399\n",
            "Epoch 99/150\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 0.0475 - accuracy: 0.9867 - val_loss: 0.4133 - val_accuracy: 0.9299\n",
            "Epoch 100/150\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.0440 - accuracy: 0.9876 - val_loss: 0.4426 - val_accuracy: 0.9299\n",
            "Epoch 101/150\n",
            "250/250 [==============================] - 5s 20ms/step - loss: 0.0281 - accuracy: 0.9927 - val_loss: 0.4030 - val_accuracy: 0.9389\n",
            "Epoch 102/150\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.0491 - accuracy: 0.9860 - val_loss: 0.4006 - val_accuracy: 0.9314\n",
            "Epoch 103/150\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.0441 - accuracy: 0.9895 - val_loss: 0.4266 - val_accuracy: 0.9354\n",
            "Epoch 104/150\n",
            "250/250 [==============================] - 5s 21ms/step - loss: 0.0538 - accuracy: 0.9856 - val_loss: 0.5235 - val_accuracy: 0.9269\n",
            "Epoch 105/150\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.0562 - accuracy: 0.9856 - val_loss: 0.4208 - val_accuracy: 0.9294\n",
            "Epoch 106/150\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.0463 - accuracy: 0.9855 - val_loss: 0.4983 - val_accuracy: 0.9319\n",
            "Epoch 107/150\n",
            "250/250 [==============================] - 5s 21ms/step - loss: 0.0417 - accuracy: 0.9897 - val_loss: 0.4126 - val_accuracy: 0.9319\n",
            "Epoch 108/150\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.0400 - accuracy: 0.9871 - val_loss: 0.4176 - val_accuracy: 0.9269\n",
            "Epoch 109/150\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.0444 - accuracy: 0.9872 - val_loss: 0.4766 - val_accuracy: 0.9279\n",
            "Epoch 110/150\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 0.0210 - accuracy: 0.9946 - val_loss: 0.4564 - val_accuracy: 0.9359\n",
            "Epoch 111/150\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 0.0307 - accuracy: 0.9931 - val_loss: 0.5129 - val_accuracy: 0.9379\n",
            "Epoch 112/150\n",
            "250/250 [==============================] - 5s 18ms/step - loss: 0.0396 - accuracy: 0.9910 - val_loss: 0.4772 - val_accuracy: 0.9374\n",
            "Epoch 113/150\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 0.0462 - accuracy: 0.9882 - val_loss: 0.4362 - val_accuracy: 0.9314\n",
            "Epoch 114/150\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.0654 - accuracy: 0.9846 - val_loss: 0.4164 - val_accuracy: 0.9324\n",
            "Epoch 115/150\n",
            "250/250 [==============================] - 5s 21ms/step - loss: 0.0354 - accuracy: 0.9914 - val_loss: 0.4306 - val_accuracy: 0.9319\n",
            "Epoch 116/150\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.0503 - accuracy: 0.9910 - val_loss: 0.4629 - val_accuracy: 0.9314\n",
            "Epoch 117/150\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.0553 - accuracy: 0.9866 - val_loss: 0.4027 - val_accuracy: 0.9304\n",
            "Epoch 118/150\n",
            "250/250 [==============================] - 5s 20ms/step - loss: 0.0365 - accuracy: 0.9905 - val_loss: 0.4800 - val_accuracy: 0.9269\n",
            "Epoch 119/150\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.0424 - accuracy: 0.9911 - val_loss: 0.4319 - val_accuracy: 0.9399\n",
            "Epoch 120/150\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.0392 - accuracy: 0.9932 - val_loss: 0.4486 - val_accuracy: 0.9379\n",
            "Epoch 121/150\n",
            "250/250 [==============================] - 5s 21ms/step - loss: 0.0423 - accuracy: 0.9885 - val_loss: 0.5011 - val_accuracy: 0.9309\n",
            "Epoch 122/150\n",
            "250/250 [==============================] - 4s 18ms/step - loss: 0.0407 - accuracy: 0.9907 - val_loss: 0.5198 - val_accuracy: 0.9304\n",
            "Epoch 123/150\n",
            "250/250 [==============================] - 4s 18ms/step - loss: 0.0379 - accuracy: 0.9896 - val_loss: 0.4146 - val_accuracy: 0.9394\n",
            "Epoch 124/150\n",
            "250/250 [==============================] - 5s 21ms/step - loss: 0.0351 - accuracy: 0.9912 - val_loss: 0.4616 - val_accuracy: 0.9329\n",
            "Epoch 125/150\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.0342 - accuracy: 0.9920 - val_loss: 0.4966 - val_accuracy: 0.9374\n",
            "Epoch 126/150\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.0273 - accuracy: 0.9929 - val_loss: 0.6433 - val_accuracy: 0.9314\n",
            "Epoch 127/150\n",
            "250/250 [==============================] - 5s 20ms/step - loss: 0.0562 - accuracy: 0.9872 - val_loss: 0.4254 - val_accuracy: 0.9359\n",
            "Epoch 128/150\n",
            "250/250 [==============================] - 4s 18ms/step - loss: 0.0275 - accuracy: 0.9922 - val_loss: 0.4502 - val_accuracy: 0.9389\n",
            "Epoch 129/150\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 0.0355 - accuracy: 0.9891 - val_loss: 0.4678 - val_accuracy: 0.9349\n",
            "Epoch 130/150\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 0.0339 - accuracy: 0.9915 - val_loss: 0.4541 - val_accuracy: 0.9379\n",
            "Epoch 131/150\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.0403 - accuracy: 0.9912 - val_loss: 0.4606 - val_accuracy: 0.9239\n",
            "Epoch 132/150\n",
            "250/250 [==============================] - 5s 22ms/step - loss: 0.0352 - accuracy: 0.9907 - val_loss: 0.3947 - val_accuracy: 0.9399\n",
            "Epoch 133/150\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.0559 - accuracy: 0.9869 - val_loss: 0.4667 - val_accuracy: 0.9259\n",
            "Epoch 134/150\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.0474 - accuracy: 0.9895 - val_loss: 0.4177 - val_accuracy: 0.9379\n",
            "Epoch 135/150\n",
            "250/250 [==============================] - 5s 22ms/step - loss: 0.0385 - accuracy: 0.9916 - val_loss: 0.4902 - val_accuracy: 0.9324\n",
            "Epoch 136/150\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.0409 - accuracy: 0.9910 - val_loss: 0.5765 - val_accuracy: 0.9259\n",
            "Epoch 137/150\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.0527 - accuracy: 0.9884 - val_loss: 0.5184 - val_accuracy: 0.9299\n",
            "Epoch 138/150\n",
            "250/250 [==============================] - 5s 21ms/step - loss: 0.0274 - accuracy: 0.9919 - val_loss: 0.4948 - val_accuracy: 0.9329\n",
            "Epoch 139/150\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.0334 - accuracy: 0.9909 - val_loss: 0.6300 - val_accuracy: 0.9294\n",
            "Epoch 140/150\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.0533 - accuracy: 0.9881 - val_loss: 0.4739 - val_accuracy: 0.9314\n",
            "Epoch 141/150\n",
            "250/250 [==============================] - 5s 21ms/step - loss: 0.0487 - accuracy: 0.9895 - val_loss: 0.4813 - val_accuracy: 0.9329\n",
            "Epoch 142/150\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.0391 - accuracy: 0.9901 - val_loss: 0.5234 - val_accuracy: 0.9304\n",
            "Epoch 143/150\n",
            "250/250 [==============================] - 4s 18ms/step - loss: 0.0378 - accuracy: 0.9910 - val_loss: 0.5068 - val_accuracy: 0.9324\n",
            "Epoch 144/150\n",
            "250/250 [==============================] - 5s 19ms/step - loss: 0.0259 - accuracy: 0.9931 - val_loss: 0.5364 - val_accuracy: 0.9329\n",
            "Epoch 145/150\n",
            "250/250 [==============================] - 4s 17ms/step - loss: 0.0390 - accuracy: 0.9911 - val_loss: 0.4860 - val_accuracy: 0.9354\n",
            "Epoch 146/150\n",
            "250/250 [==============================] - 5s 20ms/step - loss: 0.0399 - accuracy: 0.9927 - val_loss: 0.6284 - val_accuracy: 0.9274\n",
            "Epoch 147/150\n",
            "250/250 [==============================] - 4s 18ms/step - loss: 0.0431 - accuracy: 0.9895 - val_loss: 0.5365 - val_accuracy: 0.9389\n",
            "Epoch 148/150\n",
            "250/250 [==============================] - 4s 16ms/step - loss: 0.0311 - accuracy: 0.9922 - val_loss: 0.6151 - val_accuracy: 0.9319\n",
            "Epoch 149/150\n",
            "250/250 [==============================] - 5s 20ms/step - loss: 0.0605 - accuracy: 0.9887 - val_loss: 0.5048 - val_accuracy: 0.9379\n",
            "Epoch 150/150\n",
            "250/250 [==============================] - 4s 18ms/step - loss: 0.0305 - accuracy: 0.9921 - val_loss: 0.4733 - val_accuracy: 0.9364\n"
          ]
        }
      ],
      "source": [
        "dp_model_1024 = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(1024, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "\n",
        "    tf.keras.layers.Dense(10, activation='softmax'),\n",
        "])\n",
        "\n",
        "print(dp_model_1024.summary())\n",
        "\n",
        "dp_model_1024.compile(optimizer='adam',\n",
        "                 loss='sparse_categorical_crossentropy',\n",
        "                 metrics='accuracy')\n",
        "\n",
        "history = dp_model_1024.fit(X_train,\n",
        "                    y_train,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    epochs=150,\n",
        "                    batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vixIujkv30nA"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OryjKao931GU",
        "outputId": "e105bdaf-3387-4366-a5fa-11fc6d6edf25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 0s 4ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       blues       0.94      0.94      0.94       208\n",
            "   classical       0.93      0.98      0.96       203\n",
            "     country       0.88      0.90      0.89       186\n",
            "       disco       0.91      0.94      0.93       199\n",
            "      hiphop       0.99      0.92      0.95       218\n",
            "        jazz       0.94      0.91      0.92       192\n",
            "       metal       0.96      0.99      0.98       204\n",
            "         pop       0.95      0.94      0.95       180\n",
            "      reggae       0.94      0.95      0.95       211\n",
            "        rock       0.91      0.88      0.90       197\n",
            "\n",
            "    accuracy                           0.94      1998\n",
            "   macro avg       0.94      0.94      0.94      1998\n",
            "weighted avg       0.94      0.94      0.94      1998\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_val_predict = np.argmax(dp_model_1024.predict(X_val), axis=1)\n",
        "\n",
        "# Evaludation\n",
        "report = classification_report(y_val, y_val_predict, target_names=encoder.classes_)\n",
        "print(report)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
